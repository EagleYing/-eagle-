## 第二章 模型评估与选择

### 误差与过拟合

- `error rate` 错误率 = 分类错误的样本数占总样本数的比例：$E = a / m$

- `accuracy ` 精度 = 1 - 错误率：$1 - a/m$

- 误差：

  | 误差名            | 使用范围                   |
  | ----------------- | -------------------------- |
  | 误差              | 实际输出与样本真实输出之间 |
  | 训练误差/经验误差 | 在训练集上的误差           |
  | 泛化误差          | 在新样本上的误差           |

- 过拟合 `overfitting`

- 欠拟合 `underfitting`

  比如对一张带有锯齿的绿色树叶进行学习，过拟合的结果是认为所有树叶都必须带有锯齿，欠拟合的结果是绿的的都为树叶。

### 评估方法

对机器学习的泛化误差进行评估，需要一个 **测试集** ，用测试集上的 **测试误差** 多位泛化误差的近似。**测试样本尽可能不出现在训练集中**

#### 留出法 hold-out

将数据集 $D$ 划分为两个互斥集合训练集 $S$ 和测试集 $T$, $D = S \cup T , S \cap T = \varnothing$

- 常规划分比例：2/3 ～ 4/5
- 两个集合尽可能保持数据分布一致
- 由于划分的随机性，单次的留出法结果往往不够稳定，需多次随机划分，重复实验取平均值

#### 交叉验证 cross validation 

将数据集划分为 k 个大小相似的互斥子集 $D = D_1 \cup D_2 \cup … \cup D_k, D_i \cap D_j = \varnothing (i \neq j)$ 

- 子集数据分布保持一致
- 进行k次训练和测试，自身作为测试集，k-1 个座位训练集
- 得到 k 次结果取平均
- 需多次随机划分，重复 p 次实验取平均值，叫 **p次k折交叉验证**
- 当 **k = 样本数** 时为 **留一法LOO**，Leave-One-Out

#### 自助法  bootstrapping

以上两种方法评估模型使用的训练集比 $D$ 小，于是训练样本不同会导致估计偏差，自助法为了减小这一影响

- $D$ 有 m 个样本，从中抽取一个拷贝到 $D'$ 并放回 $D$ (下次采样还有可能被采样到)，在 m 次采样中样本始终不被采样的概率极限
  $$
  \lim_{m \mapsto \infty} (1 - \frac{1}{m})^m \mapsto \frac{1}{e} \thickapprox 0.368
  $$

- 所以 $D$ 中大约有36.8%的样本没有出现在 $D'$ 中，将 $D'$ 作为训练集，$D-D'$ 作为测试集

- **包外估计** 有 3/1 的没有在训练集中出现的样本用于测试

自助法在数据集较小，难以有效划分训练集/测试集时很有用，数据集足够多时前两者比较好。

#### 调参

`parameter tuning`，学习算法中有许多参数 `parameter` 需要设定。

### 性能评估

有了评估方法，我们还需要一个衡量模型泛化能力的评估标准 `performance measure`。

#### 均方误差 MSE

回归中常用的性能度量方法

- 样本集 $D = \{(x_1, y_1), (x_2, y_2), … , (x_m, y_m) \}$
- 预测结果 $f(x)$ 与真实标记 $y$

均方误差
$$
E(f; D) = \frac{1}{m}\sum_{i = 1}^m (f(x_i) - y_i)^2
$$

#### 错误率与精度

错误率定义
$$
E(f; D) = \frac{1}{m} \sum_{i = 1}^m \mathbb{I}(f(x_i) \neq y_i)
$$
精度
$$
acc(f; D) = \frac{1}{m} \sum_{i = 1}^m \mathbb{I}(f(x_i) = y_i) \\
= 1 - E(f; D)
$$

#### 查准率、查全率、F1

查准率 `precision`: 检索出的信息中有多少比例用户感兴趣；

查全率 `recall`: 用户感兴趣的信息中有多少被检索出来；

对于二分类问题：**样例总数 = TP + FP + TN + FN**

<table>
  <tr>
        <td></td>
        <td>实际值</td>
        <td>预测值</td>
        <td>全称</td>
    </tr>
    <tr>
        <td>TP</td>
        <td>Positive</td>
        <td>Positive</td>
       <td>True Positive</td>
    </tr>
  <tr>
        <td>FP</td>
        <td>Negative</td>
        <td>Positive</td>
       <td>False Positive</td>
    </tr>
  <tr>
        <td>FN</td>
        <td>Positive</td>
        <td>Negative</td>
       <td>False Negative</td>
    </tr>
  <tr>
        <td>TN</td>
        <td>Negative</td>
        <td>Negative</td>
       <td>True Negative</td>
    </tr>
</table>

分类结果 **混淆矩阵** 如下表所示

<table>
  <tr>
        <td rowspan="2">真实情况</td>    
        <td colspan="2">预测结果</td>  
    </tr>
    <tr>
        <td>正例</td> 
        <td>反例</td> 
   </tr>
    <tr>
        <td >正例</td>
      <td>TP（真正例）</td>
      <td>FN（假反例）</td>
    </tr>
  <tr>
        <td >反例</td>
      <td>FP（假正例）</td>
      <td>TN（真反例）</td>
    </tr>
</table>

查准率
$$
P = \frac{TP}{TP + FP}
$$
查全率
$$
R = \frac{TP}{TP + FN}
$$
可以看出两者是矛盾量，一般来说P高往往R低。两者关系用 **P-R曲线** 描述，平衡点 **BEP**

![](https://ws2.sinaimg.cn/large/006tNc79ly1g2ags1x6qcj30dh09cq3h.jpg)

#### F1度量

$$
F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{m(样例总数) + TP - TN}
$$

对于查准率与查全率的重视程度不同，可以设定偏好 $\beta$，$\beta = 1$ 为标准F1；$\beta > 1$ 查全率影响更大；$0 <\beta < 1$ 查准率影响更大。
$$
F_{\beta} = \frac{(1 + \beta^2) \times P \times R}{(\beta^2 \times P) + R}
$$

#### 多二维混淆矩阵

对于 n 个二分类混淆矩阵综合考察查全率和查准率的方法：

1. 计算个混淆矩阵的 $P, R$，再求平均，得到宏查准率、宏查全率、宏F1：
   $$
   macro{-}P = \frac{1}{n} \sum_{i = 1}^n P_i \\
   macro{-}R = \frac{1}{n} \sum_{i = 1}^n R_i \\
   macro{-}F1 = \frac{2 \times {macro{-}P} \times macro{-}R} {macro{-}P + macro{-}R}
   $$

2. 对混淆矩阵各元素平均再基于这些计算出微查准率、微查全率、微F1:
   $$
   macro{-}P = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}\\
   macro{-}R = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}\\
   macro{-}F1 = \frac{2 \times {macro{-}P} \times macro{-}R} {macro{-}P + macro{-}R}
   $$
